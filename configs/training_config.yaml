# Training Configuration for Llama 3.2 3B Fine-tuning

# Model settings
model:
  name: "meta-llama/Llama-3.2-3B"  # HuggingFace model (auto-downloads)
  max_seq_length: 1024  # Reduced from 2048 for speed
  
# QLoRA 4-bit quantization for speed and memory efficiency
quantization:
  load_in_4bit: true   # Enable 4-bit QLoRA
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: true

# LoRA configuration (Optimized for 12GB VRAM)
lora:
  r: 16                    # LoRA rank
  lora_alpha: 32           # LoRA scaling factor (2x rank)
  target_modules:          # 4 attention heads
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
  lora_dropout: 0.05
  bias: "none"
  task_type: "CAUSAL_LM"

# Training hyperparameters (OPTIMIZED to finish in <12 hours on Google Colab T4)
training:
  output_dir: "./checkpoints"
  num_train_epochs: 0.5                    # 50% of data, finishes in ~9-10 hours
  per_device_train_batch_size: 1           # Minimum for T4 15GB without gradient checkpointing
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16          # Effective batch size = 16 (maintained)
  learning_rate: 2.0e-4                    # Standard for QLoRA fine-tuning
  weight_decay: 0.01
  warmup_ratio: 0.02                       # Minimal warmup for speed
  lr_scheduler_type: "cosine"
  
  # Optimization
  optim: "paged_adamw_8bit"         # 8-bit paged optimizer for QLoRA
  max_grad_norm: 1.0
  
  # Memory optimization
  gradient_checkpointing: false  # Disabled for speed
  fp16: false
  bf16: true  # Use bfloat16
  
  # Logging and evaluation (MINIMAL for speed)
  logging_steps: 100                       # Log every 100 steps
  eval_strategy: "steps"
  eval_steps: 3000                         # Eval only 3 times during training
  save_strategy: "steps"
  save_steps: 3000                         # Save only 3 times
  save_total_limit: 2                      # Keep only 2 checkpoints
  
  # Data loading (OPTIMIZED)
  dataloader_num_workers: 2                # Parallel data loading
  dataloader_pin_memory: true              # Faster GPU transfer
  dataloader_prefetch_factor: 2            # Prefetch batches
  
  # Reproducibility
  seed: 42
  
  # Performance optimization
  torch_compile: false  # Keep false (compilation overhead not worth it)

# Dataset paths
data:
  tokenized_dir: "tokenized_data_1024"  # Relative path (works on Colab and local)
  
# Weights & Biases (optional)
wandb:
  enabled: false
  project: "narrator-llama32-3b"
  run_name: "llama32-3b-children-stories"
  
# GPU settings
device:
  cuda_visible_devices: "0"  # Use first GPU